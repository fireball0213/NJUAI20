# 多智能体第一次作业

###### 201300086 史浩男 人工智能学院

<img src="./多智能体第一次作业.assets/image-20230416164223504.png" alt="image-20230416164223504" style="zoom:33%;" />

##### 1、室内控制温度Agent

- **环境**: 物理环境，状态是不同的温度，温度完全可观察，没有不确定性，可能多Agent（室内空间过大，不同位置都要有Agent），序贯的（调节温度后会影响下一时刻温度），连续的（温度连续变化）
- **动作库**: 测试当前温度，控制空调升温，控制空调降温。前提条件是测温和控温设备正常，供电正常
- **设计目标**: 保持室内不同区域的温度都稳定在预设值或预设区间

##### 2、扫地机器人 Agent

- **环境**: 物理环境，房间内的垃圾和障碍物，状态是房间内各个位置的清洁程度，环境是部分可观察的 （视觉盲区)，有不确定性的 ， 单 Agent ，序贯的 (每次扫地都会影响下一次扫地)，连续的 (时间和空间连续变化).
- **动作库**: 前后左右移动，吸尘，检测障碍物，检测垃圾。前提条件：电量充足，没有损坏
- **设计目标**: 尽可能清洁房间，避免损坏自己或其他物体，避免伤害人

##### 3、围棋Agent

- **环境**: 物理环境：棋盘。状态是棋盘上棋子的分布。环境是完全可观察的，没有不确定性的 ， 单 Agent ，序贯的 (每次落子都会影响棋局)，离散的
- **动作库**: 运算状态得分，落子。前提条件：未达到棋局结束条件
- **设计目标**: 赢得棋局



<img src="./多智能体第一次作业.assets/image-20230416164231561.png" alt="image-20230416164231561" style="zoom:33%;" />

#### (1)

一个纯反应式 Agent的行动函数是 $ \mathrm{f}: \mathrm{E} \rightarrow \mathrm{Ac} $
构造一个标准 Agent B，它的行动函数是  $\mathrm{g}: \mathcal{R}^{\mathrm{E}} \rightarrow \mathrm{Ac}$ ，定义为

$τ\left(\left(\mathrm{e}_{0}，\mathrm{a}_{0}，\mathrm{e}_{1}，\mathrm{a}_{1}，\ldots，\mathrm{e}_{\mathrm{n}}\right)\right)=\mathrm{f}\left(\mathrm{e}_{\mathrm{n}}\right)$

则对于任何环境 $ E n v=\left\langle E，e_{0}，\tau\right\rangle ,
有  \mathcal{R}(\mathrm{A}，\mathrm{Env})=\mathrm{R}(\mathrm{B}，\mathrm{Env}) $

即两个 Agent 是完全行为等价的



#### (2)

假设环境 $Env = \langle \left\{\mathrm{e}_{0}，\mathrm{e}_{1}\right\}，e_0 ,\tau \rangle$


我们构造一个标准 Agent $\mathrm{A} $ ，并假设存在一个纯反应式 Agent B 与  $\mathrm{A} $ 对于  $\operatorname{Env} $ 行为等价

- 状态转移函数为  $\tau\left(\left(\mathrm{e}_{0}，\mathrm{a}_{0}\right)\right)=\left\{\mathrm{e}_{0}\right\}，\tau\left(\left(\mathrm{e}_{0}，\mathrm{a}_{0}，\mathrm{e}_{0}，\mathrm{a}_{1}\right)\right)=\left\{\mathrm{e}_{1}\right\}$ ，其余情况  $\tau(\mathrm{r})=\emptyset$ 
- 行动函数定义为$  \mathrm{~h}\left(\left(\mathrm{e}_{0}，\mathrm{a}_{0}，\mathrm{e}_{0}\right)\right)=\mathrm{a}_{1}$ ，其余情况  $\mathrm{h}(\mathrm{r})=\mathrm{a}_{0} $.

则 $ \left(\mathrm{e}_{0}，\mathrm{a}_{0}，\mathrm{e}_{0}，\mathrm{a}_{1}，\mathrm{e}_{1}，\mathrm{a}_{0}\right) \in \mathcal{R}(\mathrm{A}，\mathrm{Env}),\left(\mathrm{e}_{0}，\mathrm{a}_{0}，\mathrm{e}_{0}，\mathrm{a}_{1}，\mathrm{e}_{1}，\mathrm{a}_{0}\right) \in \mathcal{R}(\mathrm{B}，\mathrm{Env})  $

- 输入分别为 $ \left(\mathrm{e}_{0}\right) $ 与 $ \left(\mathrm{e}_{0}，\mathrm{a}_{0}，\mathrm{e}_{0}\right)  时  \mathrm{B}  $的行动函数  $\mathrm{f}\left(\mathrm{e}_{0}\right)=\mathrm{f}\left(\mathrm{e}_{0}\right)=\mathrm{a}_{0}，\mathrm{f}(\mathrm{e}_{0}，\mathrm{a}_{0}，\mathrm{e}_{0})=\mathrm{f}\left(\mathrm{e}_{0}\right)=\mathrm{a}_{1} $，与纯反应式Agent决策完全基于当前状态的定义产生矛盾.

由反证法，证毕



<img src="./多智能体第一次作业.assets/image-20230416164244350.png" alt="image-20230416164244350" style="zoom:33%;" />

假设环境 $Env = \langle \left\{\mathrm{e}_{0}\right\}，e_0 ,\tau \rangle$

此时关于运行的效用函数可以定义为$\mathrm{u}\left(\mathrm{e}_{0}\right)=0,\mathrm{u}(\mathrm{e}_{0}，\mathrm{a}_{i}，\mathrm{e}_{0})=1$

假设可以通过与状态有关的效用函数定义：

输入分别为 $ \left(\mathrm{e}_{0}\right) $ 与 $ \left(\mathrm{e}_{0}，\mathrm{a}_{i}，\mathrm{e}_{0}\right)   $时的效用函数  $\mathrm{u}\left(\mathrm{e}_{0}\right)=\mathrm{u}(\mathrm{e}_{0}，\mathrm{a}_{i}，\mathrm{e}_{0})$，产生矛盾

由反证法，证毕

<img src="./多智能体第一次作业.assets/image-20230416164323563.png" alt="image-20230416164323563" style="zoom:50%;" />
$$
EU(Ag_1,Env)=0.2\times8+0.2\times7+0.6\times4=5.4\\
EU(Ag_2,Env)=0.2\times8+0.3\times2+0.5\times5=4.7
$$
因此最优Agent是$Ag_1$

<img src="./多智能体第一次作业.assets/image-20230416164346060.png" alt="image-20230416164346060" style="zoom:33%;" />

无需讨论Dirt(x,y)，因为在计算new时，必然清扫过(x,y)位置的垃圾

```
def new(DB,p):
	new_DB_p=[p.In(x,y)]#(x,y)当前位置
	if p.Facing(d) not in DB：#d为当前方向
		new_DB_p.append(Facing(d))
	return new_DB_p
```



<img src="./多智能体第一次作业.assets/image-20230416164353010.png" alt="image-20230416164353010" style="zoom:33%;" />

增加一个动作：turnLeft，表示逆时针旋转90°

清扫规则不变，见ppt

补全导航规则:
$$
\operatorname{In}(0,0) \wedge \mathrm{F} \operatorname{acing}(  north  ) \wedge \neg \operatorname{Dirt}(0,0) \rightarrow \operatorname{Do}(  forward  ) 
\\
\operatorname{In}(0,1) \wedge \mathrm{F} \operatorname{acing}(  north  ) \wedge \neg \operatorname{Dirt}(0,1) \rightarrow \operatorname{Do}(  forward  ) 
\\
\operatorname{In}(0,2) \wedge \mathrm{F} \operatorname{acing}(  north  ) \wedge \neg \operatorname{Dirt}(0,2) \rightarrow \operatorname{Do}(  turn  ) 
\\
\operatorname{In}(0,2) \wedge \mathrm{F}  acing(east  ) \rightarrow  Do(forward)
\\
\operatorname{In}(1,2) \wedge \mathrm{F} \operatorname{acing}(  east  ) \rightarrow \operatorname{Do}(  turn  ) 
\\
\operatorname{In}(1,2) \wedge \mathrm{F} \operatorname{acing}(  south  ) \wedge \neg \operatorname{Dirt}(1,2) \rightarrow \operatorname{Do}  (forward)
\\
\operatorname{In}(1,1) \wedge \mathrm{F} \operatorname{acing}(  south  ) \wedge \neg \operatorname{Dirt}(1,1) \rightarrow \operatorname{Do}  (forward)
\\
In  (1,0) \wedge \mathrm{F}  acing(south  ) \wedge \neg \operatorname{Dirt}(1,0) \rightarrow\operatorname{Do}(  turnLeft  )  

\\
\operatorname{In}(1,0) \wedge \mathrm{F}  acing (east  ) \rightarrow  Do(forward)
\\
\operatorname{In}(2,0) \wedge \mathrm{F} \operatorname{acing}(  east  ) \rightarrow  \operatorname{Do}(  turnLeft  )
\\
\operatorname{In}(2,0) \wedge \mathrm{F} \operatorname{acing}(  north  ) \wedge \neg \operatorname{Dirt}(2,0) \rightarrow \operatorname{Do}(  forward  ) 
\\
\operatorname{In}(2,1) \wedge \mathrm{F} \operatorname{acing}(  north  ) \wedge \neg \operatorname{Dirt}(2,1) \rightarrow \operatorname{Do}(  for ward  ) 
\\
\operatorname{In}(2,2) \wedge \mathrm{F} \operatorname{acing}(  north  ) \wedge \neg \operatorname{Dirt}(2,2) \rightarrow \operatorname{Do}(  turn  ) 
\\
\operatorname{In}(2,2) \wedge \mathrm{F} \operatorname{acing}(  east  ) \rightarrow \operatorname{Do}(  turn  )
$$
直观、优美、紧凑的定义我没有找到，只能主观处理

我认为解决方案很清洗，因此直观、紧凑，但看上去并不优美

<img src="./多智能体第一次作业.assets/image-20230416164400071.png" alt="image-20230416164400071" style="zoom:33%;" />

每个非转弯位置需要1条规则，转弯位置需要两条或4条，总共20个转弯位，约共130条规则

##### 一般决策机制：

清扫规则（优先级最高）

$\operatorname{In}(\mathrm{x}，\mathrm{y}) \wedge \operatorname{Dirt}(\mathrm{x}，\mathrm{y}) \rightarrow \operatorname{Do}( clean  ) $

其他：
$$
\operatorname{In}(2 \mathrm{k}, \mathrm{m}) \wedge \mathrm{F} \operatorname{acing}(  north  ) \wedge \neg \operatorname{Dirt}(2 \mathrm{k}, \mathrm{m}) \rightarrow \operatorname{Do}(  turn  ) 
\\
\operatorname{In}(2 \mathrm{k}, \mathrm{m}) \wedge \mathrm{F} \operatorname{acing}(  east  ) \rightarrow \operatorname{Do}(  forward  ) 
\\
\operatorname{In}(2 \mathrm{k}+1, n-1) \wedge \mathrm{F}  acing  (  east  ) \rightarrow \mathrm{Do}(  turn  ) 
\\
\operatorname{In}(2 \mathrm{k}+1,0) \wedge \mathrm{F} \operatorname{acing}(  south  ) \wedge \neg \operatorname{Dirt}(2 \mathrm{k}+1,0) \rightarrow   \mathrm{Do}(\operatorname{turnLeft}) 
\\
\operatorname{In}(2 \mathrm{k}+1,0) \wedge \mathrm{F} \operatorname{acing}(  east  ) \rightarrow \mathrm{Do}(  forward  ) 
\\
\operatorname{In}(2 \mathrm{k}, 0) \wedge \mathrm{F}  acing  (  east  ) \rightarrow \mathrm{Do}(  turnLeft  ) 
\\
\operatorname{In}(2 \mathrm{k}, 0) \wedge \mathrm{F} \operatorname{acing}(\operatorname{south}) \wedge \neg \operatorname{Dirt}(2 \mathrm{k}, 0) \rightarrow \operatorname{Do}(\operatorname{turn}) 
\\
\operatorname{In}(2 \mathrm{k}, 0) \wedge \mathrm{F}  acing  (  west  ) \rightarrow \mathrm{Do}(  forward)
\\
\operatorname{In}(2 \mathrm{k}+1,0) \wedge \mathrm{F} \operatorname{acing}(  west  ) \rightarrow \operatorname{Do}(  turn  ) 
\\
\operatorname{In}(2 \mathrm{k}+1, n-1) \wedge \mathrm{F} \operatorname{acing}(  north  ) \wedge \neg \operatorname{Dirt}(2 \mathrm{k}+1, n-1) \rightarrow  Do  (  turnLeft  ) 
\\
\operatorname{In}(2 \mathrm{k}+1, n-1) \wedge \mathrm{F}  acing  (  west  ) \rightarrow \mathrm{Do}(  forward  ) 
\\
\operatorname{In}(2 \mathrm{k}, \mathrm{m}) \wedge \mathrm{F}  acing (west  ) \rightarrow \mathrm{Do}(  turnLeft  ) 

\\
\operatorname{In}(\mathrm{x}, \mathrm{y}) \wedge \mathrm{F} \operatorname{acing}(  north  ) \wedge \neg \operatorname{Dirt}(\mathrm{x}, \mathrm{y}) \rightarrow \operatorname{Do}(  forward  ) 
\\
\operatorname{In}(\mathrm{x}, \mathrm{y}) \wedge \mathrm{F} \operatorname{acing}(\operatorname{south}) \wedge \neg \operatorname{Dirt}(\mathrm{x}, \mathrm{y}) \rightarrow \operatorname{Do}(  forward)
$$




<img src="./多智能体第一次作业.assets/image-20230416164415843.png" alt="image-20230416164415843" style="zoom:33%;" />

一共有5个Agent：

##### SnowWhite

- $SnowWhite(ask) [give]$: 可接收的消息符号为 ask，可发送的消息符号为 give
- $\odot \operatorname{ask}(\mathrm{x}) \Rightarrow \diamond \operatorname{give}(\mathrm{x}) $ : 如果x昨天要糖了，则将来会把糖给x
- $\operatorname{give}(\mathrm{x}) \wedge \operatorname{give}(\mathrm{y}) \Rightarrow(\mathrm{x}=\mathrm{y})$  : 如果把糖给了  x和 y ，则他们定是同一个人，即只会把糖给一个人

##### eager

- $eager (give)  [  ask  ] $ : 可接收的消息的符号为 give，可发送的消息的符号为 ask
- $start  \Rightarrow \operatorname{ask}  (eager)$: 在系统启动后，他会立即发送一条要求糖的消息.
- $\odot \operatorname{give}(  eager  ) \Rightarrow \operatorname{ask}(  eager)$: 如果收到了糖，就会再次发送要求糖的消息

##### greedy

- $greedy (give) [ask$]: 可接收的消息的符号为 give，可发送的消息的符号为 ask
- $start  \Rightarrow \square \operatorname{ask}(  greedy )$: 在系统启动后，他会不断发送要求糖的消息

##### courteous 

- $courteous(give) [ask]$: 可接收的消息的符号为 give，可发送的消息的符号为 ask
- $((\neg \operatorname{ask}(  courteous  )  Sgive(eager  )) \wedge 
     (\neg \operatorname{ask}(  courteous)Sgive(greedy  )) \Rightarrow \operatorname{ask}(  courteous)$: 如果 eager 和 greedy 没有要求糖，他才会发送一条要求糖的消息.

##### shy

- $\operatorname{shy}  (give)  [  ask]$ : 可接收的消息的符号为 give，可发送的消息的符号为 ask
- $start  \Rightarrow \diamond \operatorname{ask}(\operatorname{shy}) $ : 在系统启动后，他将会发送一条要求糖的消息.
- $\odot \operatorname{ask}(\mathrm{x}) \Rightarrow \neg \operatorname{ask}(\operatorname{shy}) $ : 如果有其他消费者要求糖，他就不会要求糖.
- $\odot \operatorname{give}(\operatorname{shy}) \Rightarrow \diamond \operatorname{ask}(\operatorname{shy})$  : 如果之前得到了糖，则将来还会继续要求糖.
    

<img src="./多智能体第一次作业.assets/image-20230416164422889.png" alt="image-20230416164422889" style="zoom:33%;" />

1. ##### Forward:
 
 $$
 &\operatorname{Forward}(x, y, d) \\
 pre & \operatorname{In}(\mathrm{x}, \mathrm{y}) \wedge \mathrm{F} \operatorname{acing}(\mathrm{d}) \\
 del & \operatorname{In}(\mathrm{x}, \mathrm{y}) \\
 add & \operatorname{In}\left(\mathrm{x}^{\prime}, \mathrm{y}^{\prime}\right)
 $$
2. ##### Clean:
 
 $$
 &\operatorname{Clean}(\mathrm{x}, \mathrm{y}) \\
 pre & \quad \operatorname{In}(\mathrm{x}, \mathrm{y}) \wedge \operatorname{Dirt}(\mathrm{x}, \mathrm{y}) \\
 del & \operatorname{Dirt}(\mathrm{x}, \mathrm{y}) \\
 add & \emptyset
 $$
 
 
3. ##### Turn:
 
 $$
 &\operatorname{Turn}(\mathrm{d}) \\
 pre & \quad \mathrm{F} \operatorname{acing}(\mathrm{d}) \\
 del &F acing(d)\\
 add & \quad  F acing  \left(d^{\prime}\right)
 $$
 
 

<img src="./多智能体第一次作业.assets/image-20230416164450983.png" alt="image-20230416164450983" style="zoom:33%;" /><img src="./多智能体第一次作业.assets/image-20230416164507001.png" alt="image-20230416164507001" style="zoom:33%;" />



<img src="./多智能体第一次作业.assets/image-20230418204714759.png" alt="image-20230418204714759" style="zoom:67%;" />

<img src="./多智能体第一次作业.assets/image-20230416164524510.png" alt="image-20230416164524510" style="zoom: 50%;" />

#### (a) 变量 B，D 和 I:

- B代表 belief，即 Agent 所持有的信念集合，包含了 Agent 当前对世界的认知，代表环境
- D代表 desire，即 Agent 的愿望集合，表示 Agent 希望实现的状态或行为
- I代表 intention，即 Agent 当前的意图集合，表示 Agent 在当前时刻要实现的目标

#### (b) 感知  $\rho $ :

感知是 Agent 从外部环境中收集信息的方式，获取当前时刻 Agent 所处的环境状态

#### (c)  $\operatorname{brf}(\ldots)  $函数:

 $\mathrm{B}:=\operatorname{brf}(\mathrm{B}，\rho)  $：信念修正函数，用于更新 Agent 的信念集合

#### (d) $ \operatorname{options}(\ldots)  $函数:

$ D:=\operatorname{options}(\mathrm{B}，\mathrm{I})$  ：选项生成函数，产生可能的选项或愿望集合 D

#### (e)$ filter  (\ldots) $ 函数:

$ I:=\operatorname{filter}(B，D，I)  $：过滤函数，用于从愿望集合  D  中过滤掉 Agent 在当前时刻难以完成的愿望，进而生成当前的意图集合 I，也即从竞争的选项中做出 "最佳" 的选择

#### (f) $ \operatorname{plan}(\ldots) $ 函数:

$ \pi:=\operatorname{plan}(\mathrm{B}，\mathrm{I}) $规划算法，用于根据 Agent 的信念集合  B  和当前的意图集合  $\mathrm{I} $生成一个规划 $ \pi $，以实现当前的意图 I

#### (g)$ sound  (\ldots) $ 函数:

$ \operatorname{sound}(\pi，I，B) $ 函数是一个布尔函数，代表在给定 B 时，$ \pi $ 是实现 I 的正确的规划。用于判断 Agent 所制定的规划 $ \pi $ 是否合理且可行，以确保 Agent 能够在其执行规划的过程中不会违背其信念集合 B 和意图集合 I

#### (h) $succeeded  (\ldots) $ 函数和 $impossible  (\ldots)$  函数:

$succeeded  (\mathrm{I}，\mathrm{B})  $函数和$ impossible(I，B) $函数是用于判断当前的意图集合I是否是已经成功或者是不可能成功
$succeeded (I，B) $函数判断 Agent 在执行规划  $\pi$  后是否成功实现了其意图集合  $\mathrm{I}$ ，如果成功则返回 True，否则返回 False.
$impossible  (I，B) $ 函数判断 Agent 是否无法通过任何规划实现其意图集合  I ，如果 无法实现则返回 True，否则返回 False.

#### (i) $reconsider  (\ldots)$  函数:

$reconsider (I，B) $函数是一个布尔函数，用于在 Agent 执行规划过程中，根据环境 变化改变信念集合  B  的情况下，思考当前的信念集合  B  和意图集合$  \mathrm{I} $ 间的关系，以判断是否需要调整 Agent 当前的愿望集合 D 和意图集合 I. 在其认为需要重新考虑时返回 True，不需要重新考虑时返回 False.

通过引入一个布尔函数 reconsider 决定是否重新考虑意图，就能很好地权衡重复考虑意图与否之间的平衡问题，解决专一承诺 Agent和坦率承诺 Agent的两难局面






<img src="./多智能体第一次作业.assets/image-20230416164537941-1681831610380-1.png" alt="image-20230416164537941" style="zoom:33%;" />

优先级依次从高到低：

1. if 当前网格位置有垃圾，then 打扫垃圾
2. if 当前位置在起点或者终点，then 顺时针转动  $180^{\circ}$  掉头
3. if 当前位置位于上边界且面向边界，then 顺时针转动  $90^{\circ}$ ，前进一格，再顺时针转动 $ 90^{\circ}$  
4. if 当前位置位于下边界且面向边界，then 逆时针转动  $90^{\circ}$ ，前进一格，再逆时针转动 $ 90^{\circ}$  
5. if True，then 向前移动
   
   
   
   

<img src="./多智能体第一次作业.assets/image-20230416164544777-1681831610381-3.png" alt="image-20230416164544777" style="zoom:33%;" />

#### 描述 Agent 内部状态的领域谓词：

- $\operatorname{In}(\mathrm{x}，\mathrm{y})  : Agent 位于  (\mathrm{x}，\mathrm{y})  位置;$
- $\operatorname{Sample}(\mathrm{x}，\mathrm{y}):(\mathrm{x}，\mathrm{y})  位置存在样本$
- $Obstacle  (x，y):(x，y)  位置存在障碍物;$
- $\operatorname{Base}(\mathrm{x}，\mathrm{y}):(\mathrm{x}，\mathrm{y})  位置存在基地;$
- $\operatorname{Crumb}(x，y):(x，y)  位置存在碎屑;$
- $Carry: 当前是否携带样本.$



#### 动作集合:

Turn: 改变方向

MoveRandomly: 随机移动

Drop: 放下样本

PickUp: 捡起样本;

GradientUp: 沿梯度上升方向行驶

GradientDown: 沿梯度下降方向行驶

DropCrumbs: 扔下一些碎屑

PickUpCrumb: 采集一个碎屑

#### 演绎规则：

$$
\operatorname{In}(x，y) \wedge \operatorname{Obstacle}(x，y) \rightarrow \operatorname{Do}(  Turn  ) 
\\
\operatorname{In}(x，y) \wedge  Carry  \wedge \operatorname{Base}(\mathrm{x}，\mathrm{y}) \rightarrow  Do(Drop)
\\
\operatorname{In}(\mathrm{x}，\mathrm{y}) \wedge \operatorname{Carry} \wedge \neg \operatorname{Base}(\mathrm{x}，\mathrm{y}) \rightarrow \operatorname{Do}(  DropCrumbs)  \wedge 
Do(GradientUp)
\\
\operatorname{In}(\mathrm{x}，\mathrm{y}) \wedge \operatorname{Sample}(\mathrm{x}，\mathrm{y}) \rightarrow \operatorname{Do}(\mathrm{P} \operatorname{ick} \mathrm{Up}) 
\\
\operatorname{In}(x，y) \wedge \operatorname{Crumb}(\mathrm{x}，\mathrm{y}) \rightarrow \operatorname{Do}(\mathrm{PickUpCrumb}) \wedge 
Do(GradientDown)
\\
True  \rightarrow  Do(MoveRandomly  )
$$



<img src="./多智能体第一次作业.assets/image-20230416164551847-1681831610381-2.png" alt="image-20230416164551847" style="zoom: 50%;" />

- 传感器输入 (Sensor input)：接收外部环境的信号， 并将其转化为数字信号

- 感知子系统 (Perceptual sub-system)：对传感器输入的数字信号进行处理和分析，提高对外部环境的理解和感知能力

- 模型层 (Modelling layer)：代表世界上各种各样的实体 (包括 Agent 本身和其他 Agent)， 可以预言 Agent 之间的冲突， 并产生需要完成的新目标来解决这些冲突。可以将新目标下传到规划层， 利用规划库来决定如何实现它们

- 规划层 (Planning layer)：完成 Agent 的预动行为，为了实现目标会尝试在规划库中找到一个合适的规划，并据此选择出要执行的动作

- 反应层 (Reactive layer)：反应层用情景-动作规则集实现，能对环境中发生的改变提供迅速的反应

- 控制子系统 (Control subsystem)：一个动作仲裁部件，按照控制规则集来实现，对三个决策层的输入和输出数据进行分析和控制， 可以使用各种控制算法和调节策略，实现对系统行为的优化和调整

- 执行子系统 (Action subsystem)：根据三个决策层得出来的结果，执行相应的动作和操作

- 行动 (Actions)：输出部分，代表了机器对外部环境所做出的响应

  

反应式行为的实现：根据来自感知子系统的输入，实现快速响应和高效执行，以应对突发的环境变化和紧急任务，实现即时的基本的反应动作。

预动行为的实现：模型层将新目标下传到规划层，利用规划库来决定如何实现它们。规划层考虑机器人的长期目标，环境的变化和任务的需求，以实现预动行为。



<img src="./多智能体第一次作业.assets/image-20230416164604156-1681831610381-4.png" alt="image-20230416164604156" style="zoom: 50%;" />



<img src="./多智能体第一次作业.assets/image-20230418214123589.png" alt="image-20230418214123589" style="zoom:67%;" />

##### 属性：

- 课程编号，开设时间，上课教室，开设院系，选课人数：外在属性
- 授课教师：关系

##### 属性约束：

- 课程编号，开设时间，上课教室，开设院系，选课人数：唯一，且格式规范，势约束，类别约束
- 授课教师：有范围约束，需要是老师类中的成员



<img src="./多智能体第一次作业.assets/image-20230416164617179-1681831610381-5.png" alt="image-20230416164617179" style="zoom:33%;" />

1、查询

```
(:evaluate
    :sender A
    :receiver B
    :language KIF
    :ontology 课程本体
    :reply-with q1
    :content (val (选课人数 多智能体系统)))
```

2、回复

```
(:reply
    :sender B
    :receiver A
    :language KIF
    :ontology 课程本体
    :in-reply-to q1
    :content (= (选课人数 多智能体系统) (scalar 30)))
```

<img src="./多智能体第一次作业.assets/image-20230416164623547-1681831610381-6.png" alt="image-20230416164623547" style="zoom:33%;" />

```
(stream-about
    :sender A:receiver B
    :language KIF:ontology 课程本体
    :reply-with q1
    :content 机器学习导论)
(tell
    :sender B:receiver A
    :in-reply-to q1
    :content (= (课程编号 机器学习导论) (scalar 30000150)))
(tell
    :sender B:receiver A
    :in-reply-to q1
    :content (= (授课老师 机器学习导论) 周志华))
(tell
    :sender B:receiver A
    :in-reply-to q1
    :content (= (选课人数 机器学习导论) (scalar 300)))
(eos
    :sender B:receiver A
    :in-reply-to q1)

```



<img src="./多智能体第一次作业.assets/image-20230416164630234-1681831610381-7.png" alt="image-20230416164630234" style="zoom:33%;" />

#### (a) 

发送者 A 向接收者 B 发送了一个 ask-one 类型的消息, 消息内容是用 OWL 语言来描述的, 其本体为 pizza。该消息的标识为 q1, 内容是询问 margherita 是否是一种 Pizza，以及 margherita 的顶部是否有 mozzarella

####  (b) 

发送者 A 向接收者 B 发送了一个 tell 类型的消息, 消息内容是用 OWL 语 言来描述的, 其本体为 pizza。该消息的标识为 q1，内容意思为 hawaiian 不是一种 ItalianPizza, 

<img src="./多智能体第一次作业.assets/image-20230416164637775.png" alt="image-20230416164637775" style="zoom:33%;" />



合同网包括识别、通知、竞标、授予、实现五个阶段，是通过任务共享实现有效合作的高级协议

##### 识别阶段

要求所有参与者提供身份证明

```
(send-all  
     (assert    
     :content (list :provide-identity)    
         :receiver :all    
         :language :kqml)) 
```

##### 通知阶段

所有参与者已经提供了身份证明。

使用tell语用词告知所有参与者一个竞标任务

```
(send-all  
 	( assert    
        :content (list :all-participants-identified)    
        :language :kqml))     
        
(send-all  
    ( tell    
        :content (list :bid-task "task-id")    
        :receiver :all    
        :language :kqml)) 
```

#####  竞标阶段 

参与者可以使用request语用词请求竞标任务的详细信息

使用tell语用词告知所有参与者一个合同的详细信息

参与者可以使用ask-all语用词询问其他参与者是否已经提交竞标

```
(send
  ( request
    :content (list :request-task-info "task-id")
    :receiver :task-manager
    :language :kqml))

(receive
  ((:tell
     :content (list :provide-contract-info "contract-id")
     :sender :task-manager
     :language :kqml))
   (send-all
     ( tell
       :content (list :provide-contract-info "contract-id")
       :receiver :all
       :language :kqml)))

(send-all
  ( ask-all
    :content (list :bid-submitted-ask "task-id")
    :receiver :all
    :language :kqml))
```

##### 授予阶段

竞标结束后，使用tell语用词告知所有参与者竞标结果

如果有参与者中标，则使用assert语用词陈述一个事实，即“竞标任务已授予给中标者”，并告知中标者合同的详细信息

```
(receive
  ((:tell
     :content (list :bid-result "task-id" "winner-id")
     :language :kqml))
   (send-all
     ( tell
       :content (list :bid-result "task-id" "winner-id")
       :receiver :all
       :language :kqml)))
    
(if (equal "winner-id" "my-identity")
  (progn
    (send
      ( assert
        :content (list :contract-awarded "contract-id")
        :language :kqml))
    (send
      ( tell
        :content (list :provide-contract-info "contract-id")
        :receiver "winner-id"
        :language :kqml))))
```



##### 实现阶段 

当中标者同意合同后，使用tell语用词告知所有参与者该合同已经实现

```
(receive
  ((:tell
     :content (list :contract-implemented "contract-id")
     :language :kqml))
   (send-all
     ( tell
       :content (list :contract-implemented "contract-id")
       :receiver :all
       :language :kqml))))
```

